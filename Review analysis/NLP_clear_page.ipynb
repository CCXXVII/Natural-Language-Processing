{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive Reviews\n",
    "import os;\n",
    "with open(\"./tr_polarity.pos\", 'rb') as f:\n",
    "     # reviews_pos = f.read().decode('iso-8859-9').replace('\\r', '');\n",
    "     reviews_pos = f.read().decode('cp1254').replace('\\r', '').splitlines();\n",
    "\n",
    "reviews_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative Reviews\n",
    "with open(\"./tr_polarity.neg\", 'rb') as f:\n",
    "     reviews_neg = f.read().decode('cp1254').replace('\\r', '').splitlines();\n",
    "\n",
    "reviews_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Dataset Size Equality\n",
    "assert(len(reviews_pos) == len(reviews_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size = 5331, Test Size = 533, Last Index = 5330\n",
      "Dataset slices for testing: [(0, 533), (533, 1066), (1066, 1599), (1599, 2132), (2132, 2665), (2665, 3198), (3198, 3731), (3731, 4264), (4264, 4797), (4797, 5330)]\n"
     ]
    }
   ],
   "source": [
    "# Slice the dataset for testing, cross valitation and training\n",
    "# 10% for testing\n",
    "# 90% for training\n",
    "\n",
    "data_size = len(reviews_pos)\n",
    "test_size = int(data_size * 0.1)\n",
    "print(\"Data size = %d, Test Size = %d, Last Index = %d\" % (data_size, test_size, test_size * 10));\n",
    "test_index = [(x * test_size, x * test_size + test_size) for x in range(10)];\n",
    "print(\"Dataset slices for testing: %s\" % test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4797\n"
     ]
    }
   ],
   "source": [
    "# Correction: include remaining reviews, if any, to the last index pair\n",
    "test_index[9] = (test_index[9][0], data_size);\n",
    "print(test_index[9][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for dataset splitting\n",
    "def split_dataset(data, split_indexes):\n",
    "  data_test  = data[split_indexes[0]:split_indexes[1]];\n",
    "  data_training = data[0:split_indexes[0]] + data[split_indexes[1]:len(data)];\n",
    "  return { \"test\": data_test, \"train\": data_training};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "'''\n",
    "for i in range(10):\n",
    "    splits = split_dataset(reviews_pos, test_index[i])\n",
    "    pos_test = splits['test']\n",
    "    pos_train = splits['train']\n",
    "    print(\"Pos test size: %d, \\n Last item : %s\" % (len(pos_test), pos_test[-1]) )\n",
    "    print(\"Pos train size: %d, \\n Last item : %s\" % (len(pos_train), pos_train[0]) )\n",
    "\n",
    "    splits = split_dataset(reviews_neg, test_index[i])\n",
    "    neg_test = splits['test']\n",
    "    neg_train = splits['train']\n",
    "    print(\"Neg test size: %d, \\n Last item : %s\" % (len(neg_test), neg_test[-1]) )\n",
    "    print(\"Neg train size: %d, \\n Last item : %s\" % (len(neg_train), neg_train[0]) )\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\enest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk;\n",
    "import math\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "reviews_pos_tokens = [nltk.word_tokenize(doc) for doc in reviews_pos];\n",
    "reviews_neg_tokens = [nltk.word_tokenize(doc) for doc in reviews_neg];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos test size: 533, \n",
      " Last item : ['romantik-komedi', 'severler', 'icin', 'cok', 'iyi', 'bi', 'secim', 'olur', 'cok', 'sicak', 'eglenceli', 'komik', 've', 'iyi', 'oyunculukla', 'birlesince', 'guzel', 'bi', 'film', 'cikmis', 'julia', 'roberts', 'yine', 'oyunculuk', 'dersi', 'vermis', 'tavsiye', 'ederim', '.']\n",
      "Pos train size: 4798, \n",
      " Last item : ['çok', 'fazla', 'eglenceli', 'bir', 'romantik-komedi', 'idi', '.', 'en', 'fazla', 'gösterimde', 'kalan', ',', 'filmlerden', 'birisi', 'ayrica', '....']\n",
      "Neg test size: 533, \n",
      " Last item : ['kesinlikle', 'iyi', 'bir', 'psikoloji', 'gerilim', 'bekleyenlerin', 'gerilimlerini', 'karsilamayan', 'bol', 'kliseli', 'vasatin', 'bile', 'altinda', 'bir', 'film', 'filmin', 'tek', 'bir', 'artisi', 'var', 'oda', 'john', 'cusack', 'in', 'müthis', 'oyunculugu', 'bundan', 'öte', '1408', 'de', 'hiç', 'bir', 'sey', 'yok', '.', '.']\n",
      "Neg train size: 4798, \n",
      " Last item : ['bu', 'filmin', 'neresini', 'begendiniz', 'anlamadim', 'saçma', 'sapan', 'bi', 'gerilim', 'filmi', ',', 'sadece', 'geriliyosunuz', 'ama', 'belli', 'bi', 'konu', 'yok', 'abuk', 'subuk', 'bir', 'hikaye.kimseye', 'tavsiye', 'etmem', '.']\n"
     ]
    }
   ],
   "source": [
    "# Validate test split construction\n",
    "splits = split_dataset(reviews_pos_tokens, test_index[0]);\n",
    "pos_test = splits['test'];\n",
    "pos_train = splits['train'];\n",
    "print(\"Pos test size: %d, \\n Last item : %s\" % (len(pos_test), pos_test[-1]) )\n",
    "print(\"Pos train size: %d, \\n Last item : %s\" % (len(pos_train), pos_train[0]) )\n",
    "\n",
    "splits = split_dataset(reviews_neg_tokens, test_index[0]);\n",
    "neg_test = splits['test'];\n",
    "neg_train = splits['train'];\n",
    "print(\"Neg test size: %d, \\n Last item : %s\" % (len(neg_test), neg_test[-1]) )\n",
    "print(\"Neg train size: %d, \\n Last item : %s\" % (len(neg_train), neg_train[0]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary\n",
    "pos_train_flat = [X for x in pos_train for X in x];\n",
    "V_pos = { x for x in pos_train_flat }; \n",
    "print(\"|V_pos| = %d, \\n elements: %s\" % (len(V_pos), V_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_train_flat = [X for x in neg_train for X in x];\n",
    "V_neg = { x for x in neg_train_flat };\n",
    "print(\"|V_neg| = %d, \\n elements: %s\" % (len(V_neg), V_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent 20 tokens in positive reviews: [('.', 4457), ('bir', 4103), ('film', 3209), ('..', 2350), ('çok', 2011), (',', 1880), ('ve', 1575), ('bu', 1500), ('...', 1188), ('güzel', 1150), ('iyi', 1086), ('filmi', 979), ('en', 935), ('bi', 916), ('ama', 889), ('....', 882), ('!', 693), ('kadar', 635), ('bence', 628), ('daha', 558)]\n"
     ]
    }
   ],
   "source": [
    "# 1-gram statistics, i.e. 1-gram LM for POS\n",
    "N = len(V_pos);\n",
    "LM_pos = {x:0 for x in V_pos};\n",
    "for token in pos_train_flat:\n",
    "  LM_pos[token] += 1;\n",
    "\n",
    "# Most frequent tokens\n",
    "mfreq_pos = [(x,LM_pos[x]) for x in LM_pos];\n",
    "mfreq_pos.sort(key=lambda t: t[1], reverse=1);\n",
    "\n",
    "print(\"Most frequent 20 tokens in positive reviews: %s\" % (mfreq_pos[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent 20 tokens in negative reviews: [('.', 4860), ('bir', 3533), ('film', 2715), ('..', 2340), (',', 2028), ('bu', 1884), ('çok', 1542), ('ve', 1268), ('ama', 1116), ('filmi', 1095), ('...', 988), ('kötü', 931), ('kadar', 819), ('....', 788), ('bi', 766), ('!', 724), ('daha', 631), ('ne', 595), ('en', 590), ('hiç', 558)]\n"
     ]
    }
   ],
   "source": [
    "M = len(V_neg);\n",
    "LM_neg = {x:0 for x in V_neg};\n",
    "for token in neg_train_flat:\n",
    "    LM_neg[token] += 1;\n",
    "\n",
    "# Most frequent tokens\n",
    "mfreq_neg = [(x,LM_neg[x]) for x in LM_neg];\n",
    "mfreq_neg.sort(key=lambda t: t[1], reverse=1);\n",
    "\n",
    "print(\"Most frequent 20 tokens in negative reviews: %s\" % (mfreq_neg[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first 10 elements of the positive\n",
    "print('Vocabulary: ', list(V_pos)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first 10 elements of the negative\n",
    "print('Vocabulary: ', list(V_neg)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_pos = V_pos\n",
    "vocab_neg = V_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, token in enumerate(vocab_pos):\n",
    "  print('index = %d \\t vocabulary term: %s' % (idx, token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, token in enumerate(vocab_neg):\n",
    "  print('index = %d \\t vocabulary term: %s' % (idx, token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term2idx_pos = {}\n",
    "for idx, token in enumerate(vocab_pos):\n",
    "    term2idx_pos.update({token: idx})\n",
    "\n",
    "print(term2idx_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term2idx_neg = {}\n",
    "for idx, token in enumerate(vocab_neg):\n",
    "    term2idx_neg.update({token: idx})\n",
    "\n",
    "print(term2idx_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term indexes for review 1 of positive test: [1615, 221, 1947, 7067, 8424, 8906, 2429, 9585, 5414, 12314, 1622, 5366, None, 15828, 3344, 3155, None, 15248, 13481, 3542, 1947, 9799, 5366, 17222, 5318, 13147, 12945]\n",
      "Term indexes for review 1 of negative test: [10019, 15282, 15371, None, None, 8561, 2058, 13724, None, 17467, 12783, 11476, 8927, 8805, 904, 6159, 17432, 14476]\n"
     ]
    }
   ],
   "source": [
    "#Indexes for the terms of individual documents. Documents are the test reviews.\n",
    "\n",
    "print('Term indexes for review 1 of positive test: %s' % [term2idx_pos.get(token) for token in pos_test[0]])\n",
    "print('Term indexes for review 1 of negative test: %s' % [term2idx_neg.get(token) for token in neg_test[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To access terms via index, create a new dictionary from index to terms\n",
    "idx2term_pos = {}\n",
    "for term in term2idx_pos:\n",
    "    idx = term2idx_pos.get(term)\n",
    "    idx2term_pos.update({idx: term})\n",
    "print(idx2term_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2term_neg = {}\n",
    "for term in term2idx_neg:\n",
    "    idx = term2idx_neg.get(term)\n",
    "    idx2term_neg.update({idx: term})\n",
    "print(idx2term_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18306\n",
      "19830\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab_pos))\n",
    "print(len(vocab_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in the positive training set: 4798\n",
      "Number of terms in the positive vocabulary: 18306\n"
     ]
    }
   ],
   "source": [
    "pos_train_vectors = []\n",
    "for doc in pos_train:\n",
    "    doc_vector_pos = [0] * len(vocab_pos)\n",
    "    for token in doc:\n",
    "        idx = term2idx_pos.get(token)\n",
    "        doc_vector_pos[idx] = 1\n",
    "    pos_train_vectors.append(doc_vector_pos)\n",
    "print('Number of documents in the positive training set: %d' % len(pos_train_vectors))\n",
    "print('Number of terms in the positive vocabulary: %d' % len(vocab_pos)) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in the negative training set: 4798\n",
      "Number of terms in the negative vocabulary: 19830\n"
     ]
    }
   ],
   "source": [
    "neg_train_vectors = []\n",
    "for doc in neg_train:\n",
    "    doc_vector_neg = [0] * len(vocab_neg)\n",
    "    for token in doc:\n",
    "        idx = term2idx_neg.get(token)\n",
    "        doc_vector_neg[idx] = 1\n",
    "    neg_train_vectors.append(doc_vector_neg)\n",
    "print('Number of documents in the negative training set: %d' % len(neg_train_vectors))\n",
    "print('Number of terms in the negative vocabulary: %d' % len(vocab_neg))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_test_vectors = []\n",
    "for doc in pos_test:\n",
    "    doc_vector_pos = [0] * len(vocab_pos)\n",
    "    for token in doc:\n",
    "        if token in vocab_pos:\n",
    "            idx = term2idx_pos.get(token)\n",
    "            doc_vector_pos[idx] = 1\n",
    "    pos_test_vectors.append(doc_vector_pos)\n",
    "\n",
    "neg_test_vectors = []\n",
    "for doc in neg_test:\n",
    "    doc_vector_neg = [0] * len(vocab_neg)\n",
    "    for token in doc:\n",
    "        if token in vocab_neg:\n",
    "            idx = term2idx_neg.get(token)\n",
    "            doc_vector_neg[idx] = 1\n",
    "    neg_test_vectors.append(doc_vector_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "533\n",
      "533\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_test_vectors))\n",
    "print(len(neg_test_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new vector by averaging all vectors in the pos_train_vectors\n",
    "pos_train_vector_avg = [0] * len(vocab_pos)\n",
    "for doc_vector in pos_train_vectors:\n",
    "    for idx, val in enumerate(doc_vector):\n",
    "        pos_train_vector_avg[idx] += val\n",
    "for idx, val in enumerate(pos_train_vector_avg):\n",
    "    pos_train_vector_avg[idx] = val / len(pos_train_vectors)\n",
    "\n",
    "# create a new vector bu averaging all vectors in the neg_train_vectors\n",
    "neg_train_vector_avg = [0] * len(vocab_neg)\n",
    "for doc_vector in neg_train_vectors:\n",
    "    for idx, val in enumerate(doc_vector):\n",
    "        neg_train_vector_avg[idx] += val\n",
    "for idx, val in enumerate(neg_train_vector_avg):\n",
    "    neg_train_vector_avg[idx] = val / len(neg_train_vectors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18306\n",
      "19830\n",
      "[0.00020842017507294707, 0.00020842017507294707, 0.00020842017507294707, 0.00020842017507294707, 0.0010421008753647354, 0.0006252605252188412, 0.012505210504376824, 0.00041684035014589413, 0.00020842017507294707, 0.00020842017507294707]\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_train_vector_avg))\n",
    "print(len(neg_train_vector_avg))\n",
    "print(pos_train_vector_avg[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculator = math.sqrt(sum([val**2 for val in pos_train_vector_avg]))*math.sqrt(sum([val**2 for val in neg_train_vector_avg]))\n",
    "resultList = []\n",
    "\n",
    "\n",
    "for doc_vector in pos_test_vectors:\n",
    "\n",
    "    numerator_pos = sum([val1*val2 for val1, val2 in zip(doc_vector, pos_train_vector_avg)])\n",
    "    numerator_neg = sum([val1*val2 for val1, val2 in zip(doc_vector, neg_train_vector_avg)])\n",
    "\n",
    "    pos_ratio = 0\n",
    "    neg_ratio = 0\n",
    "    cos_sim = numerator_pos / calculator\n",
    "    pos_ratio = cos_sim\n",
    "    cos_sim = numerator_neg / calculator\n",
    "    neg_ratio = cos_sim\n",
    "    if pos_ratio > neg_ratio:\n",
    "        resultList.append(1)\n",
    "    else:\n",
    "        resultList.append(0)\n",
    "\n",
    "for doc_vector in neg_test_vectors:\n",
    "\n",
    "    numerator_pos = sum([val1*val2 for val1, val2 in zip(doc_vector, pos_train_vector_avg)])\n",
    "    numerator_neg = sum([val1*val2 for val1, val2 in zip(doc_vector, neg_train_vector_avg)])\n",
    "\n",
    "    pos_ratio = 0\n",
    "    neg_ratio = 0\n",
    "    cos_sim = numerator_pos / calculator\n",
    "    pos_ratio = cos_sim\n",
    "    cos_sim = numerator_neg / calculator\n",
    "    neg_ratio = cos_sim\n",
    "    if pos_ratio > neg_ratio:\n",
    "        resultList.append(1)\n",
    "    else:\n",
    "        resultList.append(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(resultList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#term by document matrix from list of document vectors\n",
    "matrix = [[doc_vec[j] for doc_vec in pos_train_vectors] for j in range(len(vocab_pos))]\n",
    "\n",
    "print('\\t \\t d1 \\t d2 \\t d3 \\t d4 \\t d5')\n",
    "for idx, row in enumerate(matrix):\n",
    "    print(\"{}\\t  {}\\t  {}\\t {}\\t  {}\\t  {}\\t  {}\".format(idx2term_pos[idx], row[0], row[1], row[2], row[3], row[4], row[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word cloud analysis\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Word cloud for document 227 and 228\n",
    "plt.figure(figsize=(16, 16))\n",
    "\n",
    "doc = ' '.join([w for w in pos_train[227]])\n",
    "wordcloud = WordCloud().generate(doc)\n",
    "plt.subplot(1, 2, 1).set_title('Document 227')\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "doc2 = ' '.join([w for w in pos_train[228]])\n",
    "wordcloud2 = WordCloud().generate(doc2)\n",
    "plt.subplot(1, 2, 2).set_title('Document 228')\n",
    "plt.imshow(wordcloud2, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e0347718afc69be7b1c23768afdbedb062dd9a48333758dd9b559c5248491fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
